// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Mongodbatlas.Outputs
{

    [OutputType]
    public sealed class GetDataLakesResultResult
    {
        public readonly ImmutableArray<Outputs.GetDataLakesResultAwResult> Aws;
        /// <summary>
        /// The cloud provider region to which Atlas Data Lake routes client connections for data processing.
        /// * `data_process_region.0.cloud_provider` - Name of the cloud service provider.
        /// * `data_process_region.0.region` -Name of the region to which Data Lake routes client connections for data processing.
        /// </summary>
        public readonly ImmutableArray<Outputs.GetDataLakesResultDataProcessRegionResult> DataProcessRegions;
        /// <summary>
        /// The list of hostnames assigned to the Atlas Data Lake. Each string in the array is a hostname assigned to the Atlas Data Lake.
        /// </summary>
        public readonly ImmutableArray<string> Hostnames;
        public readonly string Name;
        /// <summary>
        /// The unique ID for the project to get all data lakes.
        /// </summary>
        public readonly string ProjectId;
        /// <summary>
        /// Current state of the Atlas Data Lake:
        /// </summary>
        public readonly string State;
        /// <summary>
        /// Configuration details for mapping each data store to queryable databases and collections.
        /// * `storage_databases.#.name` - Name of the database to which Data Lake maps the data contained in the data store.
        /// * `storage_databases.#.collections` -     Array of objects where each object represents a collection and data sources that map to a [stores](https://docs.mongodb.com/datalake/reference/format/data-lake-configuration#mongodb-datalakeconf-datalakeconf.stores) data store.
        /// * `storage_databases.#.collections.#.name` - Name of the collection.
        /// * `storage_databases.#.collections.#.data_sources` -     Array of objects where each object represents a stores data store to map with the collection.
        /// * `storage_databases.#.collections.#.data_sources.#.store_name` -     Name of a data store to map to the `&lt;collection&gt;`.
        /// * `storage_databases.#.collections.#.data_sources.#.default_format` - Default format that Data Lake assumes if it encounters a file without an extension while searching the storeName.
        /// * `storage_databases.#.collections.#.data_sources.#.path` - Controls how Atlas Data Lake searches for and parses files in the storeName before mapping them to the `&lt;collection&gt;`.
        /// * `storage_databases.#.views` -     Array of objects where each object represents an [aggregation pipeline](https://docs.mongodb.com/manual/core/aggregation-pipeline/#id1) on a collection.
        /// * `storage_databases.#.views.#.name` - Name of the view.
        /// * `storage_databases.#.views.#.source` -  Name of the source collection for the view.
        /// * `storage_databases.#.views.#.pipeline`- Aggregation pipeline stage(s) to apply to the source collection.
        /// </summary>
        public readonly ImmutableArray<Outputs.GetDataLakesResultStorageDatabaseResult> StorageDatabases;
        /// <summary>
        /// Each object in the array represents a data store. Data Lake uses the storage.databases configuration details to map data in each data store to queryable databases and collections.
        /// * `storage_stores.#.name` - Name of the data store.
        /// * `storage_stores.#.provider` - Defines where the data is stored.
        /// * `storage_stores.#.region` - Name of the AWS region in which the S3 bucket is hosted.
        /// * `storage_stores.#.bucket` - Name of the AWS S3 bucket.
        /// * `storage_stores.#.prefix` - Prefix Data Lake applies when searching for files in the S3 bucket .
        /// * `storage_stores.#.delimiter` - The delimiter that separates `storage_databases.#.collections.#.data_sources.#.path` segments in the data store.
        /// * `storage_stores.#.include_tags` - Determines whether or not to use S3 tags on the files in the given path as additional partition attributes.
        /// </summary>
        public readonly ImmutableArray<Outputs.GetDataLakesResultStorageStoreResult> StorageStores;

        [OutputConstructor]
        private GetDataLakesResultResult(
            ImmutableArray<Outputs.GetDataLakesResultAwResult> aws,

            ImmutableArray<Outputs.GetDataLakesResultDataProcessRegionResult> dataProcessRegions,

            ImmutableArray<string> hostnames,

            string name,

            string projectId,

            string state,

            ImmutableArray<Outputs.GetDataLakesResultStorageDatabaseResult> storageDatabases,

            ImmutableArray<Outputs.GetDataLakesResultStorageStoreResult> storageStores)
        {
            Aws = aws;
            DataProcessRegions = dataProcessRegions;
            Hostnames = hostnames;
            Name = name;
            ProjectId = projectId;
            State = state;
            StorageDatabases = storageDatabases;
            StorageStores = storageStores;
        }
    }
}
