// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.mongodbatlas.outputs;

import com.pulumi.core.annotations.CustomType;
import com.pulumi.mongodbatlas.outputs.GetDataLakeAw;
import com.pulumi.mongodbatlas.outputs.GetDataLakeDataProcessRegion;
import com.pulumi.mongodbatlas.outputs.GetDataLakeStorageDatabase;
import com.pulumi.mongodbatlas.outputs.GetDataLakeStorageStore;
import java.lang.String;
import java.util.List;
import java.util.Objects;

@CustomType
public final class GetDataLakeResult {
    /**
     * @return AWS provider of the cloud service where Data Lake can access the S3 Bucket.
     * * `aws.0.role_id` - Unique identifier of the role that Data Lake can use to access the data stores.
     * * `aws.0.test_s3_bucket` - Name of the S3 data bucket that the provided role ID is authorized to access.
     * * `aws.0.role_id` - Unique identifier of the role that Data Lake can use to access the data stores.
     * * `aws.0.test_s3_bucket` - Name of the S3 data bucket that the provided role ID is authorized to access.
     * * `aws.0.iam_assumed_role_arn` - Amazon Resource Name (ARN) of the IAM Role that Data Lake assumes when accessing S3 Bucket data stores.
     * 
     */
    private List<GetDataLakeAw> aws;
    /**
     * @return The cloud provider region to which Atlas Data Lake routes client connections for data processing.
     * * `data_process_region.0.cloud_provider` - Name of the cloud service provider.
     * * `data_process_region.0.region` -Name of the region to which Data Lake routes client connections for data processing.
     * 
     */
    private List<GetDataLakeDataProcessRegion> dataProcessRegions;
    /**
     * @return The list of hostnames assigned to the Atlas Data Lake. Each string in the array is a hostname assigned to the Atlas Data Lake.
     * 
     */
    private List<String> hostnames;
    /**
     * @return The provider-assigned unique ID for this managed resource.
     * 
     */
    private String id;
    private String name;
    private String projectId;
    /**
     * @return Current state of the Atlas Data Lake:
     * 
     */
    private String state;
    /**
     * @return Configuration details for mapping each data store to queryable databases and collections.
     * * `storage_databases.#.name` - Name of the database to which Data Lake maps the data contained in the data store.
     * * `storage_databases.#.collections` -     Array of objects where each object represents a collection and data sources that map to a [stores](https://docs.mongodb.com/datalake/reference/format/data-lake-configuration#mongodb-datalakeconf-datalakeconf.stores) data store.
     * * `storage_databases.#.collections.#.name` - Name of the collection.
     * * `storage_databases.#.collections.#.data_sources` -     Array of objects where each object represents a stores data store to map with the collection.
     * * `storage_databases.#.collections.#.data_sources.#.store_name` -     Name of a data store to map to the `&lt;collection&gt;`.
     * * `storage_databases.#.collections.#.data_sources.#.default_format` - Default format that Data Lake assumes if it encounters a file without an extension while searching the storeName.
     * * `storage_databases.#.collections.#.data_sources.#.path` - Controls how Atlas Data Lake searches for and parses files in the storeName before mapping them to the `&lt;collection&gt;`.
     * * `storage_databases.#.views` -     Array of objects where each object represents an [aggregation pipeline](https://docs.mongodb.com/manual/core/aggregation-pipeline/#id1) on a collection.
     * * `storage_databases.#.views.#.name` - Name of the view.
     * * `storage_databases.#.views.#.source` -  Name of the source collection for the view.
     * * `storage_databases.#.views.#.pipeline`- Aggregation pipeline stage(s) to apply to the source collection.
     * 
     */
    private List<GetDataLakeStorageDatabase> storageDatabases;
    /**
     * @return Each object in the array represents a data store. Data Lake uses the storage.databases configuration details to map data in each data store to queryable databases and collections.
     * * `storage_stores.#.name` - Name of the data store.
     * * `storage_stores.#.provider` - Defines where the data is stored.
     * * `storage_stores.#.region` - Name of the AWS region in which the S3 bucket is hosted.
     * * `storage_stores.#.bucket` - Name of the AWS S3 bucket.
     * * `storage_stores.#.prefix` - Prefix Data Lake applies when searching for files in the S3 bucket .
     * * `storage_stores.#.delimiter` - The delimiter that separates `storage_databases.#.collections.#.data_sources.#.path` segments in the data store.
     * * `storage_stores.#.include_tags` - Determines whether or not to use S3 tags on the files in the given path as additional partition attributes.
     * 
     */
    private List<GetDataLakeStorageStore> storageStores;

    private GetDataLakeResult() {}
    /**
     * @return AWS provider of the cloud service where Data Lake can access the S3 Bucket.
     * * `aws.0.role_id` - Unique identifier of the role that Data Lake can use to access the data stores.
     * * `aws.0.test_s3_bucket` - Name of the S3 data bucket that the provided role ID is authorized to access.
     * * `aws.0.role_id` - Unique identifier of the role that Data Lake can use to access the data stores.
     * * `aws.0.test_s3_bucket` - Name of the S3 data bucket that the provided role ID is authorized to access.
     * * `aws.0.iam_assumed_role_arn` - Amazon Resource Name (ARN) of the IAM Role that Data Lake assumes when accessing S3 Bucket data stores.
     * 
     */
    public List<GetDataLakeAw> aws() {
        return this.aws;
    }
    /**
     * @return The cloud provider region to which Atlas Data Lake routes client connections for data processing.
     * * `data_process_region.0.cloud_provider` - Name of the cloud service provider.
     * * `data_process_region.0.region` -Name of the region to which Data Lake routes client connections for data processing.
     * 
     */
    public List<GetDataLakeDataProcessRegion> dataProcessRegions() {
        return this.dataProcessRegions;
    }
    /**
     * @return The list of hostnames assigned to the Atlas Data Lake. Each string in the array is a hostname assigned to the Atlas Data Lake.
     * 
     */
    public List<String> hostnames() {
        return this.hostnames;
    }
    /**
     * @return The provider-assigned unique ID for this managed resource.
     * 
     */
    public String id() {
        return this.id;
    }
    public String name() {
        return this.name;
    }
    public String projectId() {
        return this.projectId;
    }
    /**
     * @return Current state of the Atlas Data Lake:
     * 
     */
    public String state() {
        return this.state;
    }
    /**
     * @return Configuration details for mapping each data store to queryable databases and collections.
     * * `storage_databases.#.name` - Name of the database to which Data Lake maps the data contained in the data store.
     * * `storage_databases.#.collections` -     Array of objects where each object represents a collection and data sources that map to a [stores](https://docs.mongodb.com/datalake/reference/format/data-lake-configuration#mongodb-datalakeconf-datalakeconf.stores) data store.
     * * `storage_databases.#.collections.#.name` - Name of the collection.
     * * `storage_databases.#.collections.#.data_sources` -     Array of objects where each object represents a stores data store to map with the collection.
     * * `storage_databases.#.collections.#.data_sources.#.store_name` -     Name of a data store to map to the `&lt;collection&gt;`.
     * * `storage_databases.#.collections.#.data_sources.#.default_format` - Default format that Data Lake assumes if it encounters a file without an extension while searching the storeName.
     * * `storage_databases.#.collections.#.data_sources.#.path` - Controls how Atlas Data Lake searches for and parses files in the storeName before mapping them to the `&lt;collection&gt;`.
     * * `storage_databases.#.views` -     Array of objects where each object represents an [aggregation pipeline](https://docs.mongodb.com/manual/core/aggregation-pipeline/#id1) on a collection.
     * * `storage_databases.#.views.#.name` - Name of the view.
     * * `storage_databases.#.views.#.source` -  Name of the source collection for the view.
     * * `storage_databases.#.views.#.pipeline`- Aggregation pipeline stage(s) to apply to the source collection.
     * 
     */
    public List<GetDataLakeStorageDatabase> storageDatabases() {
        return this.storageDatabases;
    }
    /**
     * @return Each object in the array represents a data store. Data Lake uses the storage.databases configuration details to map data in each data store to queryable databases and collections.
     * * `storage_stores.#.name` - Name of the data store.
     * * `storage_stores.#.provider` - Defines where the data is stored.
     * * `storage_stores.#.region` - Name of the AWS region in which the S3 bucket is hosted.
     * * `storage_stores.#.bucket` - Name of the AWS S3 bucket.
     * * `storage_stores.#.prefix` - Prefix Data Lake applies when searching for files in the S3 bucket .
     * * `storage_stores.#.delimiter` - The delimiter that separates `storage_databases.#.collections.#.data_sources.#.path` segments in the data store.
     * * `storage_stores.#.include_tags` - Determines whether or not to use S3 tags on the files in the given path as additional partition attributes.
     * 
     */
    public List<GetDataLakeStorageStore> storageStores() {
        return this.storageStores;
    }

    public static Builder builder() {
        return new Builder();
    }

    public static Builder builder(GetDataLakeResult defaults) {
        return new Builder(defaults);
    }
    @CustomType.Builder
    public static final class Builder {
        private List<GetDataLakeAw> aws;
        private List<GetDataLakeDataProcessRegion> dataProcessRegions;
        private List<String> hostnames;
        private String id;
        private String name;
        private String projectId;
        private String state;
        private List<GetDataLakeStorageDatabase> storageDatabases;
        private List<GetDataLakeStorageStore> storageStores;
        public Builder() {}
        public Builder(GetDataLakeResult defaults) {
    	      Objects.requireNonNull(defaults);
    	      this.aws = defaults.aws;
    	      this.dataProcessRegions = defaults.dataProcessRegions;
    	      this.hostnames = defaults.hostnames;
    	      this.id = defaults.id;
    	      this.name = defaults.name;
    	      this.projectId = defaults.projectId;
    	      this.state = defaults.state;
    	      this.storageDatabases = defaults.storageDatabases;
    	      this.storageStores = defaults.storageStores;
        }

        @CustomType.Setter
        public Builder aws(List<GetDataLakeAw> aws) {
            this.aws = Objects.requireNonNull(aws);
            return this;
        }
        public Builder aws(GetDataLakeAw... aws) {
            return aws(List.of(aws));
        }
        @CustomType.Setter
        public Builder dataProcessRegions(List<GetDataLakeDataProcessRegion> dataProcessRegions) {
            this.dataProcessRegions = Objects.requireNonNull(dataProcessRegions);
            return this;
        }
        public Builder dataProcessRegions(GetDataLakeDataProcessRegion... dataProcessRegions) {
            return dataProcessRegions(List.of(dataProcessRegions));
        }
        @CustomType.Setter
        public Builder hostnames(List<String> hostnames) {
            this.hostnames = Objects.requireNonNull(hostnames);
            return this;
        }
        public Builder hostnames(String... hostnames) {
            return hostnames(List.of(hostnames));
        }
        @CustomType.Setter
        public Builder id(String id) {
            this.id = Objects.requireNonNull(id);
            return this;
        }
        @CustomType.Setter
        public Builder name(String name) {
            this.name = Objects.requireNonNull(name);
            return this;
        }
        @CustomType.Setter
        public Builder projectId(String projectId) {
            this.projectId = Objects.requireNonNull(projectId);
            return this;
        }
        @CustomType.Setter
        public Builder state(String state) {
            this.state = Objects.requireNonNull(state);
            return this;
        }
        @CustomType.Setter
        public Builder storageDatabases(List<GetDataLakeStorageDatabase> storageDatabases) {
            this.storageDatabases = Objects.requireNonNull(storageDatabases);
            return this;
        }
        public Builder storageDatabases(GetDataLakeStorageDatabase... storageDatabases) {
            return storageDatabases(List.of(storageDatabases));
        }
        @CustomType.Setter
        public Builder storageStores(List<GetDataLakeStorageStore> storageStores) {
            this.storageStores = Objects.requireNonNull(storageStores);
            return this;
        }
        public Builder storageStores(GetDataLakeStorageStore... storageStores) {
            return storageStores(List.of(storageStores));
        }
        public GetDataLakeResult build() {
            final var o = new GetDataLakeResult();
            o.aws = aws;
            o.dataProcessRegions = dataProcessRegions;
            o.hostnames = hostnames;
            o.id = id;
            o.name = name;
            o.projectId = projectId;
            o.state = state;
            o.storageDatabases = storageDatabases;
            o.storageStores = storageStores;
            return o;
        }
    }
}
